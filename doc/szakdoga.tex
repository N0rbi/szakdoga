% LaTeX mintaf�jl szakdolgozat �s diplomamunk�knak az
% SZTE Informatikai Tanszekcsoportja �ltal megk�vetelt
% formai k�vetelm�nyeinek megval�s�t�s�hoz
% Modositva: 2011.04.28 Nemeth L. Zoltan
% A f�jl haszn�lat�hoz sz�ks�ges a magyar.ldf 2005/05/12 v1.5-�s vagy k�s�bbi verzi�ja
% ez let�lthet� a http://www.math.bme.hu/latex/ weblapr�l, a magyar nyelv� szed�shez
% Hasznos inform�ci�k, linekek, LaTeX leirasok a www.latex.lap.hu weboldalon vannak.
%


\documentclass[12pt]{report}

%Magyar nyelvi t�mogat�s (Babel 3.7 vagy k�s�bbi kell!)
\def\magyarOptions{defaults=hu-min}
\usepackage[magyar]{babel}

%Az �kezetes bet�k haszn�lat�hoz:
\usepackage{t1enc}% �kezetes szavak automatikus elv�laszt�s�hoz
\usepackage[latin2]{inputenc}% �kezetes szavak bevitel�hez

% A formai kovetelmenyekben megk�vetelt Times bet�t�pus hasznalata:
\usepackage{times}

%Az AMS csomagjai
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

%A fejl�c l�bl�cek kialak�t�s�hoz:
\usepackage{fancyhdr}

%Term�szetesen tov�bbi csomagok is haszn�lhat�k,
%p�ld�ul �br�k beilleszt�s�hez a graphix �s a psfrag,
%ha nincs r�juk sz�ks�g term�szetesen kihagyhat�k.
\usepackage{graphicx}
\usepackage{psfrag}

%T�telszer� k�rnyezetek defini�lhat�k, ezek most fejezetenkent egyutt szamozodnak, pl.
\newtheorem{t�t}{T�tel}[chapter]
\newtheorem{defi}[t�t]{Defin�ci�}
\newtheorem{lemma}[t�t]{Lemma}
\newtheorem{�ll}[t�t]{�ll�t�s}
\newtheorem{k�v}[t�t]{K�vetkezm�ny}

%Ha a megjegyz�sek �s a p�ldak sz�veg�t nem akarjuk d�lten szedni, akkor
%az al�bbi parancs ut�n kell �ket defini�ln�:
\theoremstyle{definition}
\newtheorem{megj}[t�t]{Megjegyz�s}
\newtheorem{pld}[t�t]{P�lda}

\usepackage{setspace}

%Marg�k:
\hoffset -1in
\voffset -1in
\oddsidemargin 35mm
\textwidth 150mm
\topmargin 15mm
\headheight 10mm
\headsep 5mm
\textheight 237mm

\setstretch{1.5}



\begin{document}

%A FEJEZETEK KEZD�OLDALAINAK FEJ ES L�BL�CE:
%a plain oldalst�lust kell �tdefini�lni, hogy ott ne legyen fejl�c:
\fancypagestyle{plain}{%
%ez mindent t�r�l:
\fancyhf{}
% a l�bl�cbe jobboldalra ker�lj�n az oldalsz�m:
\fancyfoot[R]{\thepage}
%elv�laszt� vonal sem kell:
\renewcommand{\headrulewidth}{0pt}
}

%A T�BBI OLDAL FEJ �S L�BL�CE:
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Zenesz�vegek gener�l�sa karakteralap� rekurrens neur�lis h�l�zatok seg�ts�g�vel}
\fancyfoot[R]{\thepage}


%A c�moldalra se fej- se l�bl�c nem kell:
\thispagestyle{empty}

\begin{center}
\vspace*{1cm}
{\Large\bf Szegedi Tudom�nyegyetem}

\vspace{0.5cm}

{\Large\bf Informatikai Int�zet}

\vspace*{3.8cm}


{\LARGE\bf }


\vspace*{3.6cm}
%\title{Zenesz�vegek gener�l�sa karakteralap� rekurrens neur�lis h�l�zatok 
%seg�ts�g�vel}
{\Large Diplomamunka}
% vagy {\Large Szakdolgozat}

\vspace*{4cm}

%�rtelemszer�en megv�ltoztatand�:
{\large
\begin{tabular}{c@{\hspace{4cm}}c}
\emph{K�sz�tette:}     &\emph{T�mavezet�:}\\
\bf{Kis-Szab� Norbert}  &\bf{Berend G�bor}\\
programtervez� informatika     &Sz�m�t�g�pes Algoritmusok �s\\
szakos hallgat�               &Mesters�ges Intelligencia Tansz�k\\
\end{tabular}
}

\vspace*{2.3cm}

{\Large
Szeged
\\
\vspace{2mm}
2018
}
\end{center}


%A tartalomjegyz�k:
\tableofcontents

%A \chapter* parancs nem ad a fejezetnek sorsz�mot
\chapter*{Feladatki�r�s}
%A tartalomjegyz�kben m�gis szerepeltetni kell, mint szakasz(section) szerepeljen:
\addcontentsline{toc}{section}{Feladatki�r�s}

A rekurrens neur�lis h�l�k igen sikeres �s n�pszer� megold�snak sz�m�tanak 
sz�mos nyelvtechnol�giai probl�ma megold�sa sor�n. A hallgat� feladata 
(zene)sz�vegek gener�l�s�ra k�pes karakterszint� nyelvi modellek l�trehoz�sa 
rekurrens neur�lis h�l�k seg�ts�g�vel Keras k�rnyezetben Tensorflow backend 
haszn�lata mellett. A szakdolgozat tov�bbi c�lja annak vizsg�lata, hogy a 
nyelvi modellez�s kontextus�ban milyen multi-task tanul�si (\textit{multi-task 
learning}) feladatok fogalmazhat�k meg, illetve hogy ezek milyen eredm�ny 
el�r�s�re k�pesek.

\chapter*{Tartalmi �sszefoglal�}
\addcontentsline{toc}{section}{Tartalmi �sszefoglal�}

A rekurrens neur�lis h�l�k m�r a 80-as �vekben megjelentek. Ezek olyan neur�lis 
h�l�k,
 melyek figyelembe veszik az el�z� �llapotokat a d�nt�shozatalban. A ma 
 haszn�lt rekurrens h�l�k k�z�l az lstm azaz a
long shot-term memory a legkedveltebb mind k�z�l, mert megold�st tal�l az 
rnn-ek egy alapvet� probl�m�j�ra,
a gradiensek drasztikus n�veked�s�re vagy cs�kken�s�re, melyek ellehetetlenitik 
a hossz�t�v� tanul�st. Ezt a h�l�t�pust alkalmazom dolgozatomban.

Dolgozatom c�lja lstm r�tegekkel l�trehozni egy modellt, amely k�pes megtanulni 
egy adott el�ad� zenei st�lus�t karakterek sorozat�t n�zve.
Pontosabban felteszi mag�ban a k�rd�st: "ha ezt az x hossz� sz�veget l�tom, 
vajon az el�ad� mit �rna x+1. karakternek?".
A model l�trehoz�s�ban a python nyelven el�rhet� keras �s annak h�tter�ben a tensorflow keretrendszereket haszn�lom.
Keras egy API amely elfedi a neur�lis h�l�khoz sz�ks�ges matematik�t, igy �tl�that�bb� t�ve a k�dot, tensorflow pedig
egy eszk�z mellyel g�pi tanul� szoftvereket k�nnyed�n tudsz tan�tani gyorsas�ga 
miatt, valamint
�tl�that�v� teszi a fejleszt�st a tensorboard seg�ts�g�vel, mely egy 
vizualiz�ci�s eszk�z.

Szakdolgozatomban el�sz�r ismertetem az egyszer� neur�lis h�l�kat, m�k�d�s�ket,
majd ismertetem a rekurrens h�l�kat azok haszn�t, �s kit�rek a probl�m�jukra melyet az lstm old meg.
Ezut�n ismertetem a keras keretrendszer�t, a tensorflow m�k�d�s�t �s ezen bel�l a tensorboard-ot.
Ezek ismeret�ben m�r olvashat� a tensorboard vizualiz�ci�ja, �gy megmutatom a 
tan�t�sok eredm�nyeit.

\chapter*{Bevezet�s}
\addcontentsline{toc}{section}{Bevezet�s}

%G: az AlphaGo nem �ltal�nos c�l� AI
%G: a neur�lis h�l�k imperat�v (vagy b�rmilyen) programoz�si paradigm�val val� 
%szembe�ll�t�sa el�gg� marketingszag� (kb. mint az AlphaGo-t �ltal�nos 
%intelligenci�nak nevezni) 
%https://softwareengineering.stackexchange.com/questions/350629/neural-networks-the-birth-of-a-new-programming-paradigm
%G: hivatkozni k�ne a lentebbi �ll�t�sok forr�s�t (ha j�l sejtem a Karpathy 
%software 2.0-s posztja az)
A neur�lis h�l�k egyre t�bb figyelmet kapnak a mindennapokban, elk�peszt� 
teljes�tm�nyekre k�pesek mint p�ld�ul az AlphaGo, egy �ltal�nos c�l� 
mesters�ges intelligencia k�pes volt legy�zni a vil�g legjobb go j�t�kos�t. Ezt 
az esem�nyt nem v�rt�k a k�vetkez� �vtized t�vlat�ban. M�sok �gy l�tj�k, hogy a 
neur�lis h�l�k lev�ltj�k a tradicion�lis programoz�st �s az emberek csak 
fel�gyelni fogj�k az algoritmusok m�k�d�s�t, finomhangolj�k a 
hiperparam�tereket. Rengeteg helyen m�r siker�lt is �tt�r�st el�rnie az 
imperat�v programoz�s ellen. Ilyenek p�ld�ul a g�pi l�t�s, besz�dfelismer�s, 
robotika, de rengeteg m�s ter�let v�r a h�l�k h�d�t�s�ra konstans 
sz�m�t�sig�nye �s mem�riaig�nye, k�nny� �ramk�rbe �gethet�s�ge �s agiliss�ga 
miatt. B�rmi is v�r a mesters�ges intelligenci�ra az biztos, hogy egyre t�bb 
jelent�s�ge lesz az �let�nkben.

%G: a nyelvfelismer�s m�s, az NLP ford�t�sa term�szetesnyelv-feldolgoz�s
A mesters�ges intelligencia egy fontos kutat�si ter�lete a nyelvfelismer�s 
(angolul: natural language processing, r�viden NLP). Vannak imperat�v 
algoritmusok hasonl� feladatokra, de egyre nagyobb jelent�s�get kap a feladat 
neur�lis h�l�kkal t�rt�n� megval�s�t�sa. Ilyen feladatok p�ld�ul a g�pi 
ford�t�s, chatbotok, text-to-speech. Ezek mind olyan feladatok amelyek a g�pek 
�s az emberek k�z�tti kommunik�ci�t k�nny�tik meg. 

Dolgozatom t�m�ja ebbe a t�mak�rbe tartozik, c�lja automatikus dalsz�veg 
gener�l�s az el�z� karakterek megfigyel�se alapj�n. A modell pr�b�l 
�rtelmezhet� magyar dalsz�veget gy�rtani �gy, hogy k�zben k�veti az adott 
zen�sz/zenekar st�lus�t. Mivel a karaktereket v�lasztottuk absztrakci�s 
r�tegnek az inputnak, �gy nem v�rhat� el �rhet� magyar sz�veg az outputban, j� 
eredm�nynek sz�m�t viszont, ha �rthet�nek, term�szetesnek hat a gener�lt 
sz�veg, b�r a valid�l�st az utols� fejezetben statisztikailag v�gezz�k, nem 
p�ld�k seg�ts�g�vel.


\chapter{Neur�lis h�l�k}

A neur�lis h�l�k az agyunkban el�fordul� neuronok h�l�zata. Ezek hat�rozz�k meg 
a gondolatainkat, ez alapj�n hozunk d�nt�st a minket k�r�lvev� vil�gr�l. Ahhoz, 
hogy ezt sz�m�tani lehessen, sz�ks�g van egy matematikai formul�ra, egy 
modellre. A neurol�gia jelenleg elfogadott elm�let�re �p�tj�k a mesters�ges 
neur�lis h�l�kat, �s ezeknek az alapjait ebben a fejezetben fogom ismertetni.
Minden m�ly h�l� �se az el�recsatolt mesters�ges neur�lis h�l�, �gy a dolgozatban haszn�lt rekurrens, azaz hossz� t�v� mem�ri�val rendelkez� h�l� az LSTM alapja is. A fejezet ezeket a t�m�kat fogja jobban szem�gyre venni. A tov�bbiakban neur�lis h�l�k alatt a mesters�ges neur�lis h�l�kat �rtem.

\section{Neur�lis h�l�k �s a g�pi tanul�s}
%https://www.zendesk.com/blog/machine-learning-and-deep-learning/
A m�ly tanul�s �s a g�pi tanul�s kifejez�sek a k�znyelvben gyakran �sszemos�dnak, viszont l�nyeges k�l�nbs�g van a kett� k�z�tt. A g�pi tanul�s feldolgozza az adatot, tanul a megfigyeltekb�l, �s ezt k�s�bb felhaszn�lja ha hasonl� helyzetbe ker�l. Ezt �gy �ri el hogy a programoz� egy adott programoz�si nyelven leimplement�lja az adott l�p�seket: feldolgoz�s, tanul�s, predik�l�s. Sz�val ha egy probl�m�t �szlel a rendszerben, megkeresi a probl�ma forr�s�t �s kijav�tja az adott met�dust/oszt�lyt, amely a probl�m�t el�id�zte.
 
Ezzel szemben a m�ly tanul�sban nem a r�szegys�geit k�sz�ti el� a programoz�, hanem megadja az input adatot, valamint a v�rt eredm�nyt, �s be�ll�tja a modell hiperparam�tereit, �gy hogy a tanul�s hat�sos legyen. Tulajdonk�ppen a m�ly tanul�s r�szhalmaza a g�pi tanul�snak, mert ugyanazt a c�lt szolg�lj�k, viszont a m�ly tanul�sban nem azt kell meghat�rozni, hogy milyen alrendszerei vannak az adott rendszernek, hanem megadjuk az $(x,y)$ p�rokat �gy, hogy $f(x)=y$ legyen, �s az \textit{f} f�ggv�nyt a g�p pr�b�lja meg k�zel�teni �gy hogy nem ismert $(x_1, y_1)$ p�rra is j�l tippeljen a h�l�.

\section{Fel�gyelt �s fel�gyelet mentes tanul�s}
%https://www.youtube.com/watch?v=lEfrr0Yr684
A tanul�s egy m�sik csoportos�t�si szempontja, hogy fel�gyelt-e a modell tanul�sa. A fel�gyelet azt jelenti jelent esetben, hogy az input adat egy c�mk�t kap, azaz megmondjuk mi a v�rt output. Ezzel szemben a fel�gyelet mentes modell c�lja automatikus inform�ci�kinyer�s az input adatb�l. 

Ilyen p�ld�ul a SOM (azaz Self Organizing Map), mely egy adathalmaz klaszteriz�l�s�t viszi v�ghez, vagy az autoencoder, mely c�lja, hogy k�dolja az inputot, �s visszaalak�tsa azt. Ennek egy felhaszn�l�si m�dja p�ld�ul az input zajmentes�t�se. 

A rekurrens neur�lis h�l�k, amit �n is fogok alkalmazni a dolgozatomban a fel�gyelt tanul�s csoportj�ba tartoznak, mivel defin�ci� szerint a rekurrencia azt mondja meg, hogy \textit{x} l�p�s ut�n mi lesz az \textit{x+1}. l�p�s, ehhez az inputban meg kell adnunk az \textit{x+1}. l�p�st, ekkor ezek lesznek a c�mk�k, melyekre a modell optimaliz�lja mag�t. Ebbe a kateg�ri�ba tatoznak p�ld�ul az el�recsatolt valamint a konvol�ci�s neur�lis h�l�k.

\section{El�recsatolt neur�lis h�l�k}

Az el�recsatolt neur�lis h�l�k olyan adatszerkezetek, melyek k�pesek megbecs�lni az \textit{f} f�ggv�nyt �gy, hogy $ y = f(x, T) $ ahol \textit{T} a h�l�nak adott legjobb eredm�nnyel becsl� hiperparam�tereket tartalmazza. A nev�t onnan kapta, hogy az inform�ci� �raml�s�nak ir�nya az inputt�l az outputig tart, �s nincsenek benne hurkok, ciklusok.

A legegyszer�bb m�k�d� neur�lis h�l� egyetlen perceptronb�l �ll, melynek van 
egy s�lya (\textit{W}) �s egy ferdes�ge (\textit{b}). Ezekb�l a neur�lis h�l� 
el��ll�tja a $ \hat{y}=Wx + b $ egyenletet, �s ezt optimaliz�lja a tanul� 
inputra �s outputra hiba-visszaterjeszt�s alkalmaz�s�val. Sajnos ennek a 
megold�snak van egy limit�ci�ja, csak line�ris regresszi� el��ll�t�s�ra k�pes. 
Erre hozt�k l�tre az aktiv�ci�s f�ggv�nyeket, melyek nem line�ris f�ggv�nyek, 
�s minden perceptron sz�m�t�s�nak az eredm�ny�re egy nem line�ris f�ggv�ny 
h�v�dik meg. Ezeket a f�ggv�nyeket aktiv�ci�s f�ggv�nyeknek nevezik. Rengeteg 
l�tezik bel�le, de itt van n�h�ny p�lda a leggyakrabban haszn�ltak k�z�l: 
\textit{reLU}, \textit{sigmoid}, \textit{tanh}. Sz�val ha hozz�adjuk az 
aktiv�ci�s f�ggv�ny \textit{g}-t az el�z� p�ld�nkhoz a k�vetkez� 
egyenl�s�grendszert kapjuk.
A legegyszer�bb m�k�d� neur�lis h�l� egyetlen perceptronb�l �ll, melynek van egy s�lya (\textit{W}) �s egy ferdes�ge (\textit{b}). Ezekb�l a neur�lis h�l� el��ll�tja a $ \hat{y}=Wx + b $ egyenletet, �s ezt optimaliz�lja a tanul� inputra �s outputra hiba-visszaterjeszt�s alkalmaz�s�val. 
% http://ataspinar.com/2016/12/22/the-perceptron/
	\begin{figure}
		\centering
		\includegraphics[scale=0.6]{perceptron.png}
		\caption{A perceptron}
	\end{figure}

Sajnos ennek a megold�snak van egy limit�ci�ja, csak line�ris regresszi� el��ll�t�s�ra k�pes. Erre hozt�k l�tre az aktiv�ci�s f�ggv�nyeket, melyek nem line�ris f�ggv�nyek, �s minden perceptron sz�m�t�s�nak a eredm�ny�re egy nem line�ris f�ggv�ny h�v�dik meg. Ezeket a f�ggv�nyeket aktiv�ci�s f�ggv�nyeknek nevezik. Rengeteg l�tezik bel�le, de itt van n�h�ny p�lda a leggyakrabban haszn�ltak k�z�l: \textit{reLU}, \textit{sigmoid}, \textit{tanh}. Sz�val ha hozz�adjuk az aktiv�ci�s f�ggv�ny \textit{g}-t az el�z� p�ld�nkhoz a k�vetkez� egyenl�s�grendszert kapjuk.
\begin{displaymath}
\hat{y}= g(z),\ ahol\ z = Wx + b
\end{displaymath}

A neur�lis h�l� att�l lesz m�ly neur�lis h�l�, hogy t�bb perceptron r�teget egym�s ut�n k�t�nk. Ekkor lesz egy input r�teg, lesznek k�ztes, azaz l�thatatlan r�tegek �s lesz egy output r�teg. T�bb h�l�val m�lyebb tud�st tud szerezni egy modell, viszont feladatf�gg�, mivel van olyan feladat amely jobban teljes�t egy rejtett h�l�val mint t�bbel.

\section{Rekurrens neur�lis h�l�k}

Az emberek �j gondolatai nem t�rlik ki az el�z�eket. Nem az adott pillanatb�l 
�t�lj�k meg a helyzet�nket, vagy hozunk d�nt�st azzal kapcsolatban. Ez egy 
hasznos tulajdons�g, hiszen kev�s dolog van az �letben, ami nem egy folyamat 
r�sze. A rekurrens neur�lis h�l�k c�lja ezen folyamatok lemodellez�se, �gy 
mivel tudja mi t�rt�nt a m�ltban hossz�t�v� kapcsolatokkal is sz�m�t�sba venni 
modellj�ben. Minden egyes \textit{t} id�pillanatban az adott �llapot megkapja 
saj�t inputj�t $x_t$ valamint az el�z� id�pillanat predikci�j�nak eredm�ny�t
%G: az U val�j�ban h k�ne legyen, �s a h az nem a predikci�, hanem a rejtett 
%�llapot
 $U_{t-1}$ �s ez alapj�n a k�vetkez� egyenlettel v�gzi el a predikci�t az adott 
 perceptron: $h_t = f(Wx_t + Uh_{t-1}+b)$. 
 %G: mi�rt h_x, ha az el�bb m�g h_t volt?
 J�l l�that� hogy a $h_x$ egy az \textit{x}-t�l f�gg� egyenlet, mely egy 
 �ltal�nos probl�m�hoz vezet minket. Az elt�n� �s kirobban� gradiens 
 probl�m�j�hoz.

\subsection{Az RNN-ek probl�m�ja}

A rekurrens neur�lis h�l�k k�pesek nagyon nagy pontoss�ggal d�nteni, de hihetetlen�l neh�z �ket j�l betan�tani, els�sorban az elt�n� �s kirobban� gradiens probl�m�ja miatt. Ugyan�gy ahogy az el�recsatolt neur�lis h�l�k, a rekurrens h�l�k is hiba-visszaterjeszt�ssel optimaliz�lj�k perceptronjaikat. A probl�ma ott mutatkozik, hogy a rejtett r�tegek �sszek�ttet�sben vannak egym�ssal, �gy ha friss�teni akarjuk az egyik r�teget akkor az �sszes el�tte l�v�t friss�teni kell. Ekkor a h�l� els� r�tegei rengetegszer v�ltoznak a tanul�s sor�n, �s ha a gradiens, mellyel beszorozzuk t�l nagy vagy kicsi lesz, az �rt�ktelenn� teszi az adott perceptront, mivel az �sszes ut�na l�v� neuron visszaterjeszti a hib�j�t r�, �s ha rengetegszer szorzunk egy kis sz�mmal, vagy egy naggyal akkor az megk�zel�ti a null�t vagy a v�gtelent. Ekkor az els� neuronjaink haszn�lhatatlann� v�lnak, de mivel minden r�teg megkapja az el�z� r�teg outputj�t, �gy az utols� neuronok is �rt�k�ket vesztik. Ezekre a probl�m�kra sz�letett rengetek megold�s. Kirobban� gradiens: lev�gott hiba-visszaterjeszt�s, b�ntet�f�ggv�nyek, gradiens megf�kez�se. Elt�n� gradiens: s�lyok inicializ�l�sa, valamit az rnn-ek egy alt�pusa, amelyet a k�vetkez� alfejezetben fejtek ki, az LSTM.

\subsection{LSTM}

%G: az LSTM mint cs� el�g fur�n hangzik
%G: ide lehetne rakni egy �br�t pl. a colah blogr�l (term�szetesen a forr�s 
%megjel�l�s�vel egy�tt)
A Long Short Term Memory (LSTM) kulcsa az �gynevezett cella, egy hossz� egyenes 
cs�, melyen csak line�ris transzform�ci�kat hajtunk v�gre, �gy nem lesz 
radik�lis a v�ltoz�s a kezd� �s v�g�llapot k�z�tt, ezzel elker�lve a parci�lis 
deriv�lt kiugr�an magas vagy alacsony �rt�keit. Ez a cs� felel a hossz�t�v� 
mem�ri��rt. Ezt a cs�vet m�dos�tja az elfelejt� kapu (forget gate) mely egy 
\textit{sigmoid} aktiv�ci�s f�ggv�nyt tartalmaz� r�teg, valamint az �j 
inform�ci�t szolg�ltat� cs�, mely k�t r�szb�l �p�l fel. Az egyik egy 
\textit{sigmoid} aktiv�ci�s f�ggv�nnyel ell�tott r�teg, a m�sik pedig 
\textit{tanh}-val van ell�tva. El�sz�r a \textit{tanh} kisz�molja az �j 
inform�ci�t amelyet tov�bb�tani szeretne outputk�nt majd �tadja a 
\textit{sigmoid}-nak. A \textit{sigmoid} 0 �s 1 k�z� k�pzi le a az inputj�t, 
minden \textit{sigmoid} ut�n �ll egy elemenk�nti szorz�s oper�tor. Ez a 
konstrukci� k�pes eld�nteni, hogy mely elemnek milyen jelent�s�ge van, mennyire 
relev�ns az adott kontextusban. Ezut�n a sz�rt outputot hozz�adja a sz�rt 
hossz�t�v� mem�ri�hoz, amelyet majd a k�vetkez� iter�ci� fog megkapni 
hossz�t�v� mem�riak�nt. Az adott r�teg outputja viszont �gy gener�l�dik, hogy 
ezt a cs�vet, mely az �j hossz�t�v� mem�ri�t tartalmazza �tvezetj�k egy 
\textit{tanh}-s r�tegen, hogy -1 �s 1 k�z� ker�ljenek az �rt�kek, valamint 
alkalmazunk m�g egy \textit{sigmoid} sz�r�t a kapott �rt�ken ezzel megkapva az 
outputot, ami a k�vetkez� r�teg inputja is egyben.
Fontos megjegyezni, hogy mind a \textit{sigmoid}-ok, mind a \textit{tanh}-k neuronok, melyek tanul�s alatt optimaliz�lj�k a v�ltoz�ikat (\textit{W} �s \textit{b}) ezzel megtanulva mit kell tov�bb�tani a k�vetkez� r�tegeknek, valamint a glob�lis inform�ci�b�l (a cella) mely inform�ci� relev�ns sz�m�ra. Hiba-visszaterjeszt�skor ezek v�ltoznak radik�lisan, hisz ezeknek j�val kisebb vagy nagyobb tud lenni a gradiense, mint a line�ris transzform�ci�knak.

% LSTM verzi�kr�l p�r sz� ??

\chapter{M�lytanul� k�nyvt�rak}

%python �s a m�lytanul� k�nyvt�rai: pytorch, tf, CNTK, theano, caffe
% http://diploma.biblDeepLearning4J.u-szeged.hu/59970/ ?
Rengeteg k�nyvt�rat hoztak l�tre, melyek seg�tik a m�lytanul� modellek 
l�trehoz�s�t. El�sz�r is meg kell eml�teni, hogy az elm�lt �vekben a python 
nyelv v�lt a m�ly tanul�s de facto standardj�v�.
%G: pontosabban python interf�szeken kereszt�l haszn�lunk agyonoptimaliz�lt cpp 
%k�dokat
L�tezik m�s nyelvre is ismert k�nyvt�r pl: Java - DeepLearning4J, lua - torch, 
Matlab - Neural network toolbox, de k�ts�gtelen a python el�nye a piacon. 
Ott van p�ld�ul a Caffe, melyet a Berkeley egyetem egy tanul�ja, Yangqing Jia 
PhD tanulm�nyai sor�n k�sz�tett, �s az�ta is k�zkedvelt keretrendszer. M�sik 
p�lda a Microsoft Cognitive toolkit, melyet az �ri�sv�llalat tart karban, �gy 
plusz funkci�k�nt az Azure-t is t�mogatja. Hasonl� helyzet �ll fent a 
PyTorch-al kapcsolatban is, melyet a Facebook kezel, �s a Torch nev� lua 
framework python-ra val� portol�sa, mely t�mogatja a dinamikus h�l�kat. �gy 
mint a PyTorch, a dynet, mely a Carnegie Mellon Egyetemen sz�letett, is 
t�mogatja a dinamikus modellt.
Amikor d�nt�ttem keretrendszer v�laszt�s tekintet�ben, akkor ezekr�l le kellett 
mondanom, mivel szerettem volna haszn�lni a magas szint� API-t amelyet a Keras 
ny�jt, �s amelyre a k�vetkez� szekci�ban kit�rek. Amikor a k�dot elkezdtem 
leimplement�lni csak a TensorFlow �s a Theano volt kompatibilis a Keras-sal 
(igaz az�ta a Microsoft Cognitive Toolkit is kompatibilis vele).
Theano k�nyvt�r egy matematikai seg�deszk�z, mely gyors �s pontos sz�m�t�sokra 
k�pes, m�trix m�veleteket optimaliz�lja, valamint CPU-n �s GPU-n is futtathat�. 
Egyetlen probl�ma vele, hogy a f� karbantart�ja MILA (Montreal Institute for 
Learning Algorithms) abbahagyta a fejleszt�s�t, �s jelent�sen visszaesett a 
fejleszt�i hozz�j�rul�sok sz�ma az elm�lt f�l �vben.
TensorFlow a ma haszn�lt egyik legkedveltebb m�lytanul� k�nyvt�r, j�l 
dokument�lt, minden megtal�lhat� benne ami egy g�pi tanul�st alkalmaz� 
programnak kell. Igaz csak statikus modelleket k�pes l�trehozni, de k�nny� 
debuggolni, a hozz� lefejlesztett seg�deszk�z, a TensorBoard seg�ts�g�vel. �gy 
a v�laszt�som a TensorFlow-ra esett.

\section{Keras}
\label{keras}
A Keras, mint m�r eml�t�sre ker�lt, egy magas szint� API, melynek f� c�lja a gyors k�s�rletez�s. Ahogy a honlapjukon tal�lhat� id�zet �rja: `Az egyik legfontosabb dolog a kutat�sban, hogy min�l gyorsabban el�rj�nk az �tlett�l az eredm�nyig.'. Kulcsfontoss�g� volt a fejleszt�se sor�n a modularit�s �s a felhaszn�l� bar�ts�g, cser�be nem kapunk olyan gyors algoritmust, melyet mondjuk akkor kapn�nk, ha TensorFlow-t haszn�ln�nk, Keras n�lk�l.

A Keras, mint m�r eml�t�sre ker�lt, egy magas szint� API, melynek f� c�lja a 
gyors k�s�rletez�s. Ahogy a honlapjukon tal�lhat� id�zet �rja: `Az egyik 
legfontosabb dolog a kutat�sban, hogy min�l gyorsabban el�rj�nk az �tlett�l az 
eredm�nyig'. Kulcsfontoss�g� volt a fejleszt�se sor�n a modularit�s, �s a 
felhaszn�l�bar�ts�g, cser�be nem kapunk olyan gyors algoritmust, melyet mondjuk 
akkor kapn�nk, ha TensorFlow-t haszn�ln�nk, Keras n�lk�l.
Az API alapja a \textit{Model} oszt�ly. Ebb�l funkcion�lis programoz�s 
seg�ts�g�vel komplex gr�fokat tudunk l�trehozni, de ha csak sorfolytonosan 
szeretn�nk r�tegeket hozz�adni akkor az ebb�l lesz�rmaz�
\textit{Sequential} oszt�lyt kell alkalmazni, melynek az \textit{add} met�dusa param�ter�l egy \textit{Layer} objektumot v�r, �s hozz�adja k�vetkez� elemk�nt a modellhez. Ez ut�n a \textit{compile} f�ggv�nnyel megadhatjuk mit haszn�ljon, hibaf�ggv�nynek, ami alapj�n megmondja hogy teljes�t a modell, valamint mi legyen az optimaliz�l� f�ggv�ny, ami a hiba-visszaterjeszt�st vez�rli, valamint milyen metrik�kat haszn�ljon pl: loss(hiba�rt�k), accuracy(pontoss�g).
A compile ut�n a modell k�sz a tanul�sra. \textit{numpy} t�mb�ket v�r inputk�nt 
�s azt is ad vissza outputk�nt. A tanul�snak t�bb m�dja van, p�ld�ul a 
\textit{fit} a legegyszer�bb, k�t k�telez� param�tere van: \textit{x} �s 
\textit{y}. 
%G: le k�ne �rni, hogy mi az az x �s y
%G: korpuszra hivatkozol, de ezen a ponton m�g be se vezetted azt
Az �n esetemben viszont ez nem volt j�rhat� �t, mivel a korpusz amelyb�l tanul 
el�rheti a $(730441-100) * 101 = 73764441$ elem� multidimenzi�s t�mb�t (ezt a 
rock.txt v�laszt�ssal, 100-as szekvenci�n tudjuk el�rni). Ebben az esetben 
haszn�lhat� az alacsonyabb szint� \textit{train\_on\_batch} f�ggv�ny, �gy 
fut�sid�ben k�pes voltam az inputot kigener�lni a Keras sz�m�ra.
Att�l f�gg�en hogy milyen m�don adjuk meg az inputot, a tesztel�s is m�sk�pp m�k�dik. A \textit{fit}-hez hasonl�an tesztadatra megn�zhetj�k hogyan teljes�t modell�nk m�g nem l�tott adaton a \textit{evaluate} �s a \textit{train\_on\_batch}-hez hasonl�an m�k�dik a \textit{test\_on\_batch}. Ha szeretn�nk egy konkr�t predikci�t kapni akkor a \textit{predict} f�ggv�nyre van sz�ks�g�nk, ahol egy adott \textit{x}-re visszakapjuk az output \textit{y}-t.

\section{TensorFlow}
% https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
A TensorFlow egy a Google �ltal karbantartott keretrendszer, mely az elm�lt 
id�ben egyre jobban fejl�dik. Alapb�l Python-hoz �s c++ -hoz k�sz�tett�k, 
viszont m�r l�tezik JavaScript-es �s telefon kompatibilis (TensorFlow Lite) 
v�ltozata is.
%G: a k�vetkez� mondatban m�sodik fel�b�l hi�nyzik az �ll�tm�ny
Az alap verzi� is fejl�d�tt, hiszen m�r nem csak CPU-val, hanem GPU-val �s 
TPU-val, azaz tensor processing unit, melyet konkr�tan a TensorFlow-hoz 
k�sz�tettek. Ezen fel�l az elosztott rendszereket is t�mogatja.

%G: nem a m�veleteket optimaliz�ljuk, hanem egy modell param�tereit
A TensorFlow egy matematikai m�veletek optimaliz�ci�j��rt felel�s k�nyvt�r. A 
Python programoz�si nyelv egy szkriptnyelv. Ezzel nagyon sokat nyer�nk, hiszen 
egy sor k�d felel�s lehet t�bb sornyi c k�d�rt. Pythonban ak�r �gy is �rhatunk 
k�dot, hogy egy emberi mondattal le�rhatjuk a jelent�s�t.
%G: az assembly k�d m�k�d�s�t is le tudom �rni emberi mondattal
P�ld�ul: a,b = b,a jelentheti azt, hogy felcser�lem az a �s b v�ltoz� �rt�k�t.
%G: ford�t�s (compiling) �rtelemben nem t�rt�nik, hanem a m�r ford�tott c k�dot 
%k�nyelmesen hivogathatod a python interpreterrel
Viszont ezzel gyorsas�got vesz�t�nk, hiszen a Python sorr�l sorra ford�tja �t a 
k�dot c k�dd�. Ennek a lass�s�gnak elker�l�se �rdek�ben tal�lt�k ki a 
TensorFlow-t mely a h�tt�rben c k�dot gener�l, optimaliz�lva a m�veleteket, �gy 
a neur�lis h�l�khoz sz�ks�ges intenz�v matematikai sz�m�t�sok napokr�l �r�kra 
cs�kkenhetnek.

A keretrendszer elm�leti h�ttere, hogy a modell�nket egy adatfolyam gr�fnak 
tekinti, melyekben a folyamok tal�lkoz�si pontj�n�l transzform�ci�k t�rt�nnek. 
Az adat amely kereszt�l megy a gr�fon, egy vektor, m�s n�ven Tensor. %G: vektor 
%!= Tensor. Minden vektor tenzor, de nem minden tenzor vektor. 
%https://en.wikipedia.org/wiki/Tensor

%G: Ez az att�l f�ggetlen�l fel�t�s azt a l�tszatot kelti, mintha a matematikai 
%sz�m�t�sok �s a m�lytanul�s egym�ssal �ssze nem f�gg� dolgok lenn�nek
Att�l f�ggetlen�l, hogy matematikai sz�m�t�sokra fejlesztett�k, nagyon sok m�ly 
tanul� tartalom van benne, hibaf�ggv�nyekt�l �s optimaliz�l� f�ggv�nyekt�l 
kezdve k�sz modelleken �t. Dolgozatom kezdete �ta a Keras is r�sze lett a 
TensorFlow k�nyvt�rnak.

\subsection{TensorBoard}

Az el�z� szekci�ban eml�tett m�lytanul� k�nyvt�r egy nagyon nagy el�nye m�s 
alternat�v�kkal szemben a TensorBoard nev� be�p�tett vizualiz�ci�s eszk�z. 
Ennek seg�ts�g�vel kev�s k�ddal nagyszer� vizualiz�ci�kat figyelhet�nk meg 
tanul�s k�zben �s ut�n is. 
%G: ez a "sz�mokra van sz�ks�ge" el�g fur�n hangzik
A TensorFlow-nak sz�mokra van sz�ks�ge ahhoz, hogy a modell elk�sz�lj�n, mivel 
ez egy optimaliz�l�si feladat, viszont az embereknek neh�z felfogni hatalmas 
adatok t�bl�zat�t, erre j� a vizualiz�ci�, hogyha valami nincs rendben a 
modellel egy gyakorlott adattud�s k�nnyen �szreveheti hol rontotta el a 
modellj�t, valamint egy kezd� is megl�tja hogyha valami probl�ma van a 
modellel. Ilyen eszk�z�k a skal�ris vektorok, melyek az id� f�ggv�ny�ben 
mutatj�k meg egy tensor �rt�k�nek v�ltoz�s�t, hisztogramok, melyek az 
eloszl�s�t n�zik az adott tensor �rt�keinek, a projektorral k�pes vagy egy 
vektorteret kirajzolni, melyet PCA (Principle Component Analysis) vagy T-SNE 
(T-distributed Stochastic Neighbour Rendering), esetleg egy�ni lek�pez�ssel. A 
modell architekt�r�j�t is k�zelebbr�l szem�gyre vehetj�k, hiszen van egy olyan 
men�pont, mely egy gr�fot gener�l nek�nk az elk�sz�lt modellb�l amelyben 
l�thatjuk hogy a tensorok hogy folynak v�gig a sz�m�t�si gr�fon, azaz egy 
statikus k�pet kapunk, hogy fut�sid�ben hogyan fog v�gigfolyni az adat. Viszont 
dolgozatom megkezd�se �ta a TensorBoard egy �jabb verzi�ja m�r t�mogatja a 
dinamikus hibakeres�st is.

\chapter{Karakter alap� sz�veg modellez�s}

% https://en.wikipedia.org/wiki/Natural-language_processing
%G: "how to program computers to fruitfully process large amounts of natural 
%language data" ez nem (csak) a kommunik�ci�t jelenti
% annyira nem r�gi dolog az NLP, hogy �rdemes lenne elm�lt �vsz�zadr�l 
%besz�lni. Legyen ink�bb elm�lt �vtizedekben
A term�szetes nyelvfeldolgoz�s egy olyan tudom�ny�g, melynek c�lja az emberek �s a g�pek k�z�tti kommunik�ci� l�trehoz�sa. Erre nagyon sok algoritmus sz�letett az elm�lt �vsz�zadban, viszont az id� azt igazolta, hogy a neur�lis h�l�k legal�bb olyan j�k erre a c�lra mint a l�tez� NLP algoritmusok. 

Dolgozatomban egy karakter alap� neur�lis h�l�t hoztam l�tre, mely az el�z� karakterek alapj�n k�pes megj�solni, hogy a tanult korpusz (egy zeneszerz�) mely karakterrel folytatn� a kapott sztringet (dalsz�veget).

Az �rott sz�veg modellez�s�nek t�bb absztrakci�ja is l�tezik: karakter, token, sz� �s mondat alap�.
Ezek a balr�l jobbra egyre komplexebb egys�gek, ami azt jelenti, hogy t�bb adatra van sz�ks�g, hogy a modell relev�ns �rt�ket adjon vissza, hiszen p�ld�ul karakterb�l sokkal kevesebb van mint sz�b�l, �gy ha sz� alap� modellt haszn�ln�nk rengeteg irrelev�ns inform�ci�t kapn�nk ritka szavakr�l (b�r erre is van megold�s, melyre az els� szekci�ban kit�rek). Ez azt jelenti, hogy nagyon nagy adathalmazzal �s sz�m�t�kapacit�ssal meg�ri magasabb szint� absztrakci�t haszn�lni, viszont �n az egyszer�s�g �s a k�nny� fejleszt�s (kev�s v�rakoz�s a modellek futtat�sa k�z�tt,) miatt a karakteralap� modellez�st v�lasztottam.

\section{Preprocessz�l�s}

Minden neur�lis modell els� �s az egyik legfontosabb r�sze a preprocessz�l�s, 
azaz az adatok el�k�sz�t�se a tanul�sra. 
%G: map-re helyett mondjuk lek�pez�sre?
Jelen esetben el�sz�r is sz�ks�g�nk van egy map-re, mely a karakternek egy 
egy�ni azonos�t�t ad 0 �s az �sszes elt�r� karakter sz�ma (tov�bbiakban 
\textit{N}) k�z�tt. Ez ut�n �talak�tjuk a sz�mokat one-hot k�dol�st haszn�lva, 
azaz minden karakter kap egy \textit{N} dimenzi�s vektort, �s az 
\textit{n}-edik azonos�t� line�risan f�ggetlen az \textit{n+1}-edikt�l. Ezzel 
azt �rj�k el, hogy mik�zben a modell�nk tanul, nem pr�b�l meg rel�ci�t vonni a 
karakterek k�z�tt p�ld�ul ha nem haszn�lunk k�dol�st, �s azt mondjuk hogy a 
indexe 1 �s b indexe 2, azt is gondolhatja a modell�nk hogy b k�tszer olyan 
�rt�kes mint a, ez�rt van sz�ks�g�nk a line�ris f�ggetlens�gre, hisz akkor nem 
�llhat fenn rezonancia. % G: itt mit �rt�nk rezonancia alatt, ez valami 
%ford�t�sb�l j�n? �rdemes lenne ezt a r�szt �t�rni
%a kerasos embedding r�teg haszn�lata eset�ben ugyan�gy 1-hot reprezent�ci� 
%van, csak ezt elrejti a keras
Az �jabb megval�s�t�shoz egy m�sik m�dszert haszn�lunk, melyhez nincs sz�ks�g�nk a one-hot k�dol�sra, mert a modell meg fogja tan�tani az adott karaktert reprezent�l� vektorokat minden vektorra, de ezek nem lesznek line�risan f�ggetlenek, rel�ci�ba �ll�that�ak a karakterek, viszont ezek a rel�ci�k v�rhat�an relev�nsak lesznek, nem �gy mint az el�bb felhozott p�ld�m. Ezt a megval�s�t�st a keras \textit{Embedding} r�tege fogja megval�s�tani.

%TODO: referencia a CharEncoder oszt�lyra

\section{A modell}

A modell a Keras keretrendszer objektumaib�l �p�l fel, a szekci� ezekre t�r ki. 
Mivel dolgozatom sor�n nem volt sz�ks�gem komplex modellre, el�g volt a 
\textit{Sequential}-t haszn�lnom, melyhez a \textit{add} f�ggv�nnyel lehet 
hozz�adni r�teget. A legutolj�ra felhaszn�lt oszt�ly, melyre m�r eml�t�st 
tettem az \textit{Embedding} r�teg, melyr�l a dokument�ci� is �rja, hogy csak a 
modell els� r�tegk�nt alkalmazhat�. Ennek 3 param�ter�t adjuk meg, a sz�t�r 
m�rete, az elv�rt embedding egys�gek sz�ma, azaz mennyi dimenzi�ra szeretn�nk 
levet�teni az inputot. Ezt a sz�mot �ltal�ban �rdemes nagyobbra �ll�tani mint a 
sz�t�r m�ret�t, %G: erre van forr�s, mert akkor azt hivatkozni kellene. Am�gy 
%nagy �ltal�noss�gban ez biztos nincs �gy, mert pl. amikor sz�alakokhoz 
%tanulnak 
%embeddingeket, nem 300 f�l� el�g ritk�n mennek, m�rpedig sz�b�l j�val t�bb 
%van, 
%mint 300.
mert �gy tudnak kialakulni l�that� kapcsolatok karakterek k�z�tt, p�ld�ul a 
sz�mok �ssze tudnak csoportosulni, de ezt vizu�lisan is tudjuk k�s�bb 
szeml�ltetni.
Az \textit{Embedding} val�j�ban egy \textit{Dense} r�teg, mely param�ter�l v�r egy one-hot k�dol�ssal ell�tott vektort. Ez csak egy seg�ts�g a fejleszt� sz�m�ra, hogy levegye a terhet a v�ll�r�l, valamint a
TensorBoard haszn�lat�val vizualiz�lhatjuk a sz�t�r elemei k�z�tti 
kapcsolatokat.

Az \textit{Embedding} a Keras keretrendszeren bel�l val�j�ban egy \textit{Dense} olyan r�teg, mely param�ter�l v�r egy one-hot k�dol�ssal ell�tott vektort. Ez csak egy seg�ts�g a fejleszt� sz�m�ra, hogy levegye a terhet a v�ll�r�l, valamint a TensorBoard haszn�lat�val vizualiz�lhatjuk az sz�t�r elemei k�z�tti kapcsolatokat.

Az embedding tulajdonk�ppen egy adathalmaz elemeit pr�b�lja rel�ci�ba �ll�tani. F�leg szavak rel�ci�ba �ll�t�s�ra haszn�lj�k, de a fent eml�tett p�ld�b�l l�that�, hogy az �ltalam l�trehozott karakter alap� embedding-nek is van emberileg felfoghat� �rt�ke. Vannak nem m�ly tanul� algoritmusok sz�be�gyaz�sokra, valamint vannak el�re tan�tott adathalmazok, ha olyan programot szeretn�nk �rni amiben sz� alap� nyelvi felismer�sre van sz�ks�g. Ilyen p�ld�ul a Google �ltal karban tartott \textit{word2vec} valamint a nagy riv�lisa a \textit{GloVe}. A kett� k�z�tt a l�nyegi k�l�nbs�g hogy a \textit{word2vec} predikci�kat v�gez, ameddig a \textit{GloVe} statisztikai alapon v�gzi a sz�mol�st, viszont a motorh�ztet� alatt mindkett� a sz�veg kontextus�b�l pr�b�l meg r�j�nni az adott sz� jelent�s�re.

A m�r eml�tett \textit{Dense} r�teg egy teljesen �sszekapcsolt r�teg, az input mindegyik eleme hozz� van kapcsolva az output �sszes r�sz�hez. Ha sima neur�lis h�l�ra lenne sz�ks�g�nk ezeket kellene haszn�lnunk, k�l�nb�z� mennyis�gben, k�l�nb�z� sz�m� output egys�gekkel.

A k�vetkez� r�teg az \textit{LSTM}, azaz a rekurrencia r�sz�t k�pz� h�l�/h�l�k. Egy olyan h�l�, mely �nmag�ba csatol�dik vissza minden input karakter ut�n. A k�vetkez� param�tereket haszn�lom: \textit{units}, azaz h�ny egys�g legyen benne, hasonl�an m�k�dik mint a \textit{Dense} egys�gek, \textit{return\_sequences}, mely ha igazra van �ll�tva, mint jelen esetben, azt mondja meg hogy az �sszes �llapotot adja-e vissza, valamint a \textit{statefulness}, azaz a k�vetkez� �llapot az el�z�t vegye-e param�ter�l, ami szint�n igazra van �ll�tva.

%G: szigmoid helyett softmax, �s annak a k�plete is ideker�lhetne
Utolj�ra egy \textit{Dense} r�teget adunk meg, \textit{szigmoid} f�ggv�nnyel, melynek output dimenzi�inak meg kell egyeznie az \textit{y} dimenzi�ival, mivel ez a modell�nk outputja.

Ezzel elk�sz�l a magas szint� keras modell�nk, ez ut�n a \textit{compile} f�ggv�nnyel elk�sz�l ebb�l a TensorFlow modell�nk. De ehhez el�bb m�g meg kell eml�ten�nk a \textit{compile} met�dus �sszes sz�ks�ges param�ter�t. Ezek a hibaf�ggv�ny, az optimaliz�l� �s opcion�lis a metrik�k.

\subsection{Hibaf�ggv�ny}
% wikip�dia
A hibaf�ggv�ny a matematikai optimaliz�ci�ban, statisztik�ban, g�pi tanul�sn�l �s m�ly tanul�sn�l haszn�lt kifejez�s. Tulajdonk�pp vektorokat, m�trixokat (a mi eset�nkben tensorokat) egy val�s sz�mra vet�t le, melyet a fent eml�tett ter�leteken haszn�lnak, hogy sz�mszer�s�teni tudj�k a tanul�s �llapotait, azaz meg tudja n�zni hogy javult-e a tanul�s az adott iter�ci�ban (m�s n�ven kisebb-e a hibaf�ggv�ny �rt�ke).

A hibaf�ggv�nyt a programoz� feladata meghat�rozni, hiszen rossz hibaf�ggv�nnyel a tanul�s �rtelmetlen.
Kerasban a k�sz modell�nk compile f�ggv�ny�ben szerepel a hibaf�ggv�ny, k�telez� param�terk�nt. Mi magunk is l�trehozhatunk hibaf�ggv�nyeket, melyben param�terk�nt kapunk egy \textit{y\_true} �s egy \textit{y\_pred} tensort �s ez alapj�n kell kisz�molni a hib�t.
Ha �ltal�nos hibaf�ggv�nyre van sz�ks�g a Keras mag�ba foglal rengeteget k�z�l�k. Ezeket jelent�s�kkel egy�tt a k�vetkez� t�bl�zat fogja szeml�ltetni. Ezeket beg�pelhetj�k sztringk�nt az al�bbi t�bl�zat alapj�n, vagy a \textit{keras.losses} csomagban tal�lhat� r�juk a referencia, mellyel saj�t param�terekkel is megh�vhatjuk �ket, de a dokument�ci�ban ki van emelve, hogy ez nem aj�nlott.
%TODO leir�s
\begin{table}[!h]\label{loss}
	\caption{Keras keretrendszer el�redefini�lt hibaf�ggv�nyei}
	\begin{center}
		\begin{tabular}{r|r}
			N�v&Le�r�s\\
			\hline\hline
			mean\_squared\_error&n\\
			mean\_absolute\_error&n\\
			mean\_absolute\_percentage\_error&n\\
			mean\_squared\_logarithmic\_error&n\\
			squared\_hinge&n\\
			hinge&n\\
			categorical\_hinge&n\\
			logcosh&n\\
			categorical\_crossentropy&n\\
			sparse\_categorical\_crossentropy&n\\
			binary\_crossentropy&n\\
			kullback\_leibler\_divergence&n\\
			poisson&n\\
			cosine\_proximity&n\\
		\end{tabular}
	\end{center}
\end{table}

A mi eset�nkben a megfelel� v�laszt�s a categorical\_crossentropy lesz, mivel minden karakter egy k�l�n�ll� kateg�ria �s azt akarjuk megmondani hogy az adott kateg�ri�nak mekkora es�lye van a k�vetkez�nek lenni az input alapj�n.

\subsection{Optimaliz�l� f�ggv�ny}
%https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f
A compile f�ggv�ny m�sik k�telez� param�tere az optimaliz�l� f�ggv�ny, melyet megadhatunk sztringk�nt is valamint egy callback-el is hozz�adhatjuk a modell�nkh�z. A keras el�redefini�lt optimaliz�l�i a \textit{keras.optimizers}-ben tal�lhat�ak. Az optimaliz�l� c�lja minimaliz�lni a hibaf�ggv�ny �rt�k�t. Ezek a hibaf�ggv�ny deriv�ltj�t veszik alapul, c�lja megtal�lni azt a pontot, ahol ha a hibaf�ggv�ny�nk $f(x) = y$, akkor megkeresi azt az �rt�ket ahol $f'(x)$ �rt�ke negat�v, azaz $y$ $x$ viszonylat�ban cs�kken. Vannak esetek, mikor a m�sodik deriv�ltat is �rdemes felhaszn�lni, de ez ritka, mivel a m�sodik deriv�ltat, azaz a f�ggv�ny \textit{Hesse} m�trix�t sz�m�t�sk�lts�ges kisz�molni.

\begin{table}[!h]\label{optimizer}
	\caption{Keras keretrendszer el�redefini�lt optimaliz�l� f�ggv�nyei}
	\begin{center}
		\begin{tabular}{r|r}
			N�v&Le�r�s\\
			\hline\hline
			SGD&n\\
			RMSProp&n\\
			Adagrad&n\\
			Adadelta&n\\
			Adam&n\\
			Adamax&n\\
			Nadam&n\\
			TFOptimizer&n\\
		\end{tabular}
	\end{center}
\end{table}

\subsection{Metrik�k}

Szint�n megadhat� param�terk�nt a compile-nak a \textit{metrics}, mely egy t�mb�t v�r, melynek elemei vagy sztringek, vagy egy callback f�ggv�ny. A k�dban ez az egyetlen p�lda, ahol kihaszn�lom a callback f�ggv�nnyel val� defini�l�st, mivel a keras alapb�l nem tartalmazza a perplexit�s metrik�t. A perplexit�s a term�szetes nyelvfeldolgoz�s egy alap metrik�ja, jelent�se az, hogy a modell milyen j�l m�solja a val�di korupusz eloszl�s�t. A m�sik felhaszn�lt metrika a pontoss�g, l�nyeg�ben visszak�rj�k a kerast�l hogy a modell milyen pontosan hat�rozta meg a v�rt �rt�ket a tan�t�s sor�n. 

A metrika egy olyan egys�ge a modellnek, melyet nem haszn�l fel a tanul�s sor�n, ez csak a fejleszt� sz�m�ra ny�jt seg�ts�get, hogy meg�rtse hogyan viselkedik a modellje.

\section{A tanul�s menete}

Ott tartunk hogy van egy modell�nk, az input tan�t�sra k�sz �llapotban van, �s a modell le lett compile-olva az adott alacsony szint� nyelvre, adott esetben tensorflow-ra.

A keras t�bb m�dot is felk�n�l a modell�nk tan�t�s�ra, melyekr�l m�r sz� esett a kerasr�l sz�l� \ref{keras} fejezetben. Szem�ly szerint az alacsony szint� \textit{train\_on\_batch} megval�s�t�st v�lasztottam, mivel az els� pr�b�lkoz�s alkalm�val a fit nem bizonyult haszn�lhat�nak a hatalmas t�rmennyis�gnek amit a preprocessz�lt input adat megk�vetel. Erre haszn�lhattam volna a \textit{fit\_generator}-t, viszont mikor ezt �szrevettem m�r hatalmas refaktor�l�sra lett volna sz�ks�gem a \textit{lyrics.py} f�jlban, mivel az �sszes callback-et, melyet a \textit{fit} t�mogat meg kellett val�s�tanom a saj�t k�domban. Ezek a tensorboard vizualiz�ci�i, valamint az korai meg�ll�s.

Az early stopping, azaz korai meg�ll�s azt jelenti, hogy hab�r a tanul� adathalmazon �lland�an egyre jobb �rt�keket siker�l el�rnie, a valid�ci�s halmazon l�tszik, hogy van egy pont, ahol elkezd romlani az �rt�k. Ekkor ha hagyjuk a modellnek hogy tov�bb tanuljon egyre rosszabbul teljes�t majd a valid�ci�s halmazon, �s egyre jobb lesz a tan�t� inputon, ezt a jelens�get szeretn�nk elker�lni. Ezt egyszer�en megtehetj�k, de el�bb p�r a tan�t�shoz fontos fogalommal meg kell ismerkedn�nk.

% EPOCH, BATCH, validation, test

% early stopping megval�s�t�sa

% tensorboard logol�sok

\chapter{Tanul�s eredm�nyei}

\chapter*{Nyilatkozat}
%Egy �res sort adunk a tartalomjegyz�khez:
\addtocontents{toc}{\ }
\addcontentsline{toc}{section}{Nyilatkozat}
%\hspace{\parindent}

% A nyilatkozat sz�vege m�s titkos �s nem titkos dolgozatok eset�ben.
% Csak az egyik tipus� myilatokzatnak kell a dolgozatban szerepelni
% A ponok hely�re az adatok �rtelemszer�en behelyettes�dend�k es
% a szakdolgozat /diplomamunka szo megfeleloen kivalasztando.


%A nyilatkozat sz�vege TITKOSNAK NEM MIN�S�TETT dolgozatban a k�vetkez�:
%A pontokkal jel�lt sz�vegr�szek �rtelemszer�en a sz�vegszerkeszt�ben �s
%nem k�zzel helyettes�tend�k:

\noindent
Alul�rott \makebox[4cm]{\dotfill} szakos hallgat�, kijelentem, hogy a dolgozatomat a Szegedi Tudom�nyegyetem, Informatikai Int�zet \makebox[4cm]{\dotfill} Tansz�k�n k�sz�tettem, \makebox[4cm]{\dotfill} diploma megszerz�se �rdek�ben.

Kijelentem, hogy a dolgozatot m�s szakon kor�bban nem v�dtem meg, saj�t munk�m eredm�nye, �s csak a hivatkozott forr�sokat (szakirodalom, eszk�z�k, stb.) haszn�ltam fel.

Tudom�sul veszem, hogy szakdolgozatomat / diplomamunk�mat a Szegedi Tudom�nyegyetem Informatikai Int�zet k�nyvt�r�ban, a helyben olvashat� k�nyvek k�z�tt helyezik el.

\vspace*{2cm}

\begin{tabular}{lc}
Szeged, \today\
\hspace{2cm} & \makebox[6cm]{\dotfill} \\
& al��r�s \\
\end{tabular}


\vspace*{4cm}

%A nyilatkozat sz�vege TITKOSNAK MIN�S�TETT dolgozatban a k�vetkez�:

\noindent
Alul�rott \makebox[4cm]{\dotfill} szakos hallgat�, kijelentem, hogy a dolgozatomat a Szegedi Tudom�nyegyetem, Informatikai Int�zet \makebox[4cm]{\dotfill} Tansz�k�n k�sz�tettem, \makebox[4cm]{\dotfill} diploma megszerz�se �rdek�ben.

Kijelentem, hogy a dolgozatot m�s szakon kor�bban nem v�dtem meg, saj�t munk�m eredm�nye, �s csak a hivatkozott forr�sokat (szakirodalom, eszk�z�k, stb.) haszn�ltam fel.

Tudom�sul veszem, hogy szakdolgozatomat / diplomamunk�mat a TVSZ 4. sz. mell�klet�ben le�rtak szerint kezelik.

\vspace*{2cm}

\begin{tabular}{lc}
Szeged, \today\
\hspace{2cm} & \makebox[6cm]{\dotfill} \\
& al��r�s \\
\end{tabular}





\chapter*{K�sz�netnyilv�n�t�s}
\addcontentsline{toc}{section}{K�sz�netnyilv�n�t�s}

Ez�ton szeretn�k k�sz�netet mondani \textbf{X. Y-nak} ez�rt �s ez�rt \ldots


%% Az itrodalomjegyzek keszitheto a BibTeX segedprogrammal:
%\bibliography{diploma}
%\bibliographystyle{plain}

%VAGY "k�zzel" a k�vetkez� m�don:

\begin{thebibliography}{9}
%10-n�l kevesebb hivatkoz�s eset�n

%\begin{thebibliography}{99}
% 10-n�l t�bb hivatkoz�s eset�n

\addcontentsline{toc}{section}{Irodalomjegyz�k}

%Elso szerzok vezetekneve alapjan �b�c�rendben rendezve.


%foly�irat cikk: szerzok(k), a foly�irat neve kiemelve,
%az evfolyam felkoveren, zarojelben az evszam, vegul az oldalszamok es pont.
\bibitem{Gischer}
J. L. Gischer,
The equational theory of pomsets.
\emph{Theoret. Comput. Sci.}, \textbf{61}(1988), 199--224.

%k�nyv (szerzo(k), a k�nyv neve kiemelve, utana a kiado, a kiado szekhelye, az evszam es pont.)
\bibitem{Pin}
J.-E. Pin,
\emph{Varieties of Formal Languages},
Plenum Publishing Corp., New York, 1986.





\end{thebibliography}




\end{document}
